{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf73190c-4e92-4329-a27c-937903a5fce8",
   "metadata": {},
   "source": [
    "**Text summarization** is the process of distilling the most important information from a text document to produce a shorter version, while still retaining the key points and meaning. There are generally two approaches to text summarization: extractive and abstractive.\n",
    "\n",
    "1.  **Extractive Summarization**: In this approach, sentences or phrases are selected directly from the original text and assembled to form a summary. This method typically involves identifying important sentences based on features such as word frequency, sentence length, and position in the document.\n",
    "    \n",
    "2.  **Abstractive Summarization**: In this approach, a model generates new sentences that capture the main ideas of the original text, often rephrasing and synthesizing information. This method requires more advanced natural language processing techniques, such as sequence-to-sequence models and attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eeac04-05bc-42a8-b14d-ded974981883",
   "metadata": {},
   "source": [
    "Below is the sample code for understanding **Extractive Summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ac7bb6-97e6-4b51-938b-673b0006c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractive summarization involves selecting important sentences or phrases directly from the original text and assembling them to form a summary.\n",
      "Abstractive summarization, on the other hand, involves generating new sentences that capture the main ideas of the original text, often rephrasing and synthesizing information.\n"
     ]
    }
   ],
   "source": [
    "## Example for Extractive Summarization\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "# Sample text for summarization\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction between computers and humans through natural language. It enables computers to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP techniques are used in a wide range of applications, including machine translation, sentiment analysis, and text summarization.\n",
    "\n",
    "Text summarization is the process of distilling the most important information from a text document to produce a shorter version, while still retaining the key points and meaning. There are generally two approaches to text summarization: extractive and abstractive.\n",
    "\n",
    "Extractive summarization involves selecting important sentences or phrases directly from the original text and assembling them to form a summary. This method is relatively straightforward and relies on features such as word frequency, sentence length, and position in the document.\n",
    "\n",
    "Abstractive summarization, on the other hand, involves generating new sentences that capture the main ideas of the original text, often rephrasing and synthesizing information. This method requires more advanced natural language processing techniques, such as sequence-to-sequence models and attention mechanisms.\n",
    "\n",
    "Gensim is a popular Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It provides implementations of several text summarization algorithms, including the TextRank algorithm, which is an extractive summarization technique based on graph theory.\n",
    "\n",
    "In this example, we'll use Gensim's summarize function to perform extractive text summarization on the sample text provided above.\n",
    "\"\"\"\n",
    "\n",
    "# Perform extractive text summarization\n",
    "summary = summarize(text)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267b5e4-826b-4df0-aa4d-5fd2535e60d2",
   "metadata": {},
   "source": [
    "Below is the sample code for understanding **Abstractive Summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ff03b3-dc21-4e6b-8f92-a6292175355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text :  ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'e', 'l', 'd', ' ', 'o', 'f', ' ', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', '.']\n",
      "Vocab Set :  [' ', '(', ')', ',', '-', '.', 'I', 'L', 'N', 'P', 'S', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n",
      "Token index :  {' ': 0, '(': 1, ')': 2, ',': 3, '-': 4, '.': 5, 'I': 6, 'L': 7, 'N': 8, 'P': 9, 'S': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'l': 20, 'm': 21, 'n': 22, 'o': 23, 'p': 24, 'q': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32, 'z': 33}\n",
      "The encoder_input_data is a 3d array of size (4, X, 34), here 4 is num of sequences in input_texts, X is the max length of a sequence and 34 is num of vocab chars, hence you get  one hot vector where chars are activated for each sequence\n",
      "Encoder Input of 1st sequences and 1st char is N and index 8 is active:  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "## Prep the dataset first\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for training\n",
    "input_texts = ['Natural language processing (NLP) is a field of artificial intelligence.',\n",
    "               'It deals with the interaction between computers and humans through natural language.',\n",
    "               'NLP enables computers to understand, interpret, and generate human language.',\n",
    "               'Sequence-to-sequence models are commonly used for abstractive summarization.']\n",
    "\n",
    "target_texts = ['NLP is a field of AI.',\n",
    "                'NLP deals with interaction between computers and humans through language.',\n",
    "                'NLP enables computers to understand, interpret, and generate human language.',\n",
    "                'Seq2Seq models are used for abstractive summarization.']\n",
    "\n",
    "# Tokenize input and target texts\n",
    "input_texts = [[char for char in text] for text in input_texts]\n",
    "target_texts = [[char for char in text] for text in target_texts]\n",
    "\n",
    "print('Tokenized Text : ',input_texts[0]) # See output to get an idea\n",
    "\n",
    "# Create vocabulary sets\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "for input_text, target_text in zip(input_texts, target_texts):\n",
    "    input_chars.update(input_text)\n",
    "    target_chars.update(target_text)\n",
    "\n",
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "\n",
    "print('Vocab Set : ',input_chars) # See output to get an idea\n",
    "\n",
    "num_encoder_tokens = len(input_chars)\n",
    "num_decoder_tokens = len(target_chars)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# Create character-to-index dictionaries\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
    "\n",
    "print('Token index : ',input_token_index) # See output to get an idea\n",
    "\n",
    "# Create encoder input data\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "\n",
    "print('The encoder_input_data is a 3d array of size (4, X, 34), here 4 is num of sequences in input_texts, X is the max length of a sequence and 34 is num of vocab chars, hence you get  one hot vector where chars are activated for each sequence')\n",
    "print('Encoder Input of 1st sequences and 1st char is N and index 8 is active: ',encoder_input_data[0][0]) # See output to get an idea\n",
    "\n",
    "# Create decoder input and target data\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') # This is simiÃ¶at to encoder_input_data\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, target_text in enumerate(target_texts):\n",
    "    for t, char in enumerate(target_text):\n",
    "        # Decoder input data does not include the start token\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # Decoder target data is one timestep ahead of decoder input data and does not include the start token\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6613d662-69f4-493f-afdc-a87fd33c8405",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydot\\core.py:1753\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1753\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydot\\core.py:133\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 133\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogram_with_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworking_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1024\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1021\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1022\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1509\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1509\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1511\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1518\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\model_visualization.py:36\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydot\\core.py:1762\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1761\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{prog}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prog\u001b[38;5;241m=\u001b[39mprog)\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Visualize the model architecture\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\model_visualization.py:405\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, show_layer_activations, show_trainable, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message)\n\u001b[1;32m--> 405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    406\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor `plot_model` to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\model_visualization.py:38\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "## Model arch building and training\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Define encoder architecture\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder architecture\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Visualize the model architecture\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86741e54-a77d-46c0-abeb-253d723c98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba95ab-e3dd-47d5-a712-327483eb9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing section\n",
    "\n",
    "# Inference mode (sampling)\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "# Decode function\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Test the model\n",
    "for seq_index in range(10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
