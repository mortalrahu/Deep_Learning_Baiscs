{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0578e3-775c-42c3-b289-d03120975550",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Language Modeling** is like teaching a computer to finish your sentences. Imagine you start writing a sentence, \"The weather today is...\", and someone jumps in to complete it with \"...sunny.\" That's what a language model does; it predicts the next word(s) based on the words that come before. This model learns from a vast amount of text to understand how words naturally come together in human language.\n",
    "\n",
    "**Text Generation** takes this a step further. Using a language model, it can create entirely new sentences or even paragraphs that sound like they could have been written by a human. It's like giving the computer a theme and watching it write its own story or article. It works by utilizing algorithms and language models to process input data and generate output text. It involves training AI models on large datasets of text to learn patterns, grammar, and contextual information. These models then use this learned knowledge to generate new text based on given prompts or conditions.\n",
    "\n",
    "**Do Large Language Models (LLMs) Work on the Same Principle?**\n",
    "\n",
    "Yes, Large Language Models (LLMs) like GPT (Generative Pretrained Transformer) work on the same principle but at a much larger scale and complexity. They're trained on extensive text data, enabling them to generate more coherent, diverse, and contextually relevant text. These models have a deeper understanding of language nuances, can maintain context over longer stretches of text, and can even mimic specific writing styles.\n",
    "At the core of text generation are language models, such as GPT (Generative Pre-trained Transformer) and Google’s PaLM, which have been trained on vast amounts of text data from the internet. These models employ deep learning techniques, specifically neural networks, to understand the structure of sentences and generate coherent and contextually relevant text.\n",
    "\n",
    "During the text generation process, the AI model takes a seed input, such as a sentence or a keyword, and uses its learned knowledge to predict the most probable next words or phrases. The model continues to generate text, incorporating context and coherence, until a desired length or condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c1fa23-431d-482f-a195-66423e324653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3558e26-b66b-4627-a167-87695f788745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# The text mapped to int\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c048b6cd-0961-4bf7-b4e0-9c074449a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum length for a single input\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Create sequences\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# Split into input and target text\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Shuffle and batch the data\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee1f3d0-54c3-4a0c-9c8b-e45972607012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_7 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m65\u001b[0m)               │          \u001b[38;5;34m66,625\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = len(vocab)  # Length of the vocabulary\n",
    "embedding_dim = 256      # The embedding dimension\n",
    "rnn_units = 1024         # Number of RNN units\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    # Ensure GRU layer is aware it's stateful; actual batch size defined during training\n",
    "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "])\n",
    "\n",
    "# Due to limitations in specifying the batch size directly in the model architecture without causing errors,\n",
    "# ensure your training and generation phases accommodate the statefulness of the model.\n",
    "\n",
    "# Example: Running a single batch through the model to build it\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    # Just a forward pass to build the model\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "# Now, try printing the model summary again\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8783c8ce-ffbb-44d1-9bc4-03c0e5e402ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 699ms/step - loss: 3.0876\n",
      "Epoch 2/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 719ms/step - loss: 1.9244\n",
      "Epoch 3/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 714ms/step - loss: 1.6457\n",
      "Epoch 4/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 717ms/step - loss: 1.5101\n",
      "Epoch 5/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 719ms/step - loss: 1.4252\n",
      "Epoch 6/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 734ms/step - loss: 1.3649\n",
      "Epoch 7/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 738ms/step - loss: 1.3221\n",
      "Epoch 8/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 769ms/step - loss: 1.2866\n",
      "Epoch 9/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 811ms/step - loss: 1.2549\n",
      "Epoch 10/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 825ms/step - loss: 1.2200\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Training step\n",
    "EPOCHS=10\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdba0e04-e73f-4cff-8ef5-9910da7782f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET: you as that lies us answer a quarteran slave's tongue\n",
      "And thus I said Kate to be to enocure he,\n",
      "Procurve kill why, the dishonour news,\n",
      "Did no man did wrangling write life\n",
      "From the vault and like mine, fare you work?\n",
      "And hath charitenced my sons.\n",
      "\n",
      "Signior Grelion:\n",
      "Help me, in mistrust, ann words,\n",
      "That Edward should be pulled us.\n",
      "\n",
      "LEONTES:\n",
      "Go, cithe design, or else he is\n",
      "A thing to learn to favours in this land,\n",
      "Poor ears to Liceo oftend 'gainst Menenius\n",
      "With closely counten and suspicion\n",
      "As thought my clivaring world corrage warrant?\n",
      "Who both is fair wreth, and shalt be ng clothe ere\n",
      "doth gain both rime nights: thou shaming stumplers' record.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "An andave us, I would yield do well\n",
      "But what woe doth husband you promised\n",
      "The provost blosd behome against the easter lend the very light:\n",
      "And soon bribe and leave in bribe, our sight son\n",
      "would flesh up the tempest to his friends.\n",
      "\n",
      "VALET:\n",
      "Ay, or else you fit your part, God have young\n",
      "Both Strobt villain: I will attend thy back--\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    # Converting start_string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[c] for c in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)  # Expand to 2D for the model input, shape: (1, len(input_eval))\n",
    "\n",
    "    # List to store the generated text\n",
    "    text_generated = []\n",
    "\n",
    "    # Ensuring the internal state is reset\n",
    "    #model.reset_states()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # predictions shape is (batch_size, sequence_length, vocab_size)\n",
    "        # We use the last character from the last time step\n",
    "        predictions = predictions[:, -1, :]  # Shape: (1, vocab_size)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0,0].numpy()\n",
    "\n",
    "        # Prepare the input for the next pass\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        # Save the generated character\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate text\n",
    "print(generate_text(model, start_string=u\"JULIET: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf698f7b-bcc2-42a7-8f30-33342c82c174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
